<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Official website for ICCV 2023 paper: Multi-metrics adaptively identifies backdoors in Federated learning">
  <meta property="og:title" content="Multi-metrics adaptively identifies backdoors in Federated learning"/>
  <meta property="og:description" content="Official website for ICCV 2023 paper: Multi-metrics adaptively identifies backdoors in Federated learning"/>
  <meta property="og:url" content="https://siquanhuang.github.io/Multi-metrics/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/overview.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Multi-metrics adaptively identifies backdoors in Federated learning">
  <meta name="twitter:description" content="Official website for ICCV 2023 paper: Multi-metrics adaptively identifies backdoors in Federated learning">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/overview.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Federated learning, Backdoor attack, Distance-based defense, Multi-metrics">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- MathJax Configuration -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <title>Multi-metrics adaptively identifies backdoors in Federated learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Multi-metrics adaptively identifies backdoors in Federated learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Siquan Huang</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yijiang Li</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Chong Chen</a></a><sup>1</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Leyu Shi</a><sup>1</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Ying Gao</a><sup>1</sup>,</span>
                  </span>


                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>South China University of Technology, <sup>2</sup>Johns Hopkins University<br>
                      <div class="column has-text-centered">
                        <span class="link-block">
                          <a href="https://iccv2023.thecvf.com/" target="_blank"
                             class="external-link button is-normal is-rounded"
                             style="background: linear-gradient(90deg, #185a9d 0%, #43cea2 100%); color: #fff; font-weight: 800; box-shadow: 0 8px 32px rgba(24,90,157,0.18); border: none; transition: all 0.25s cubic-bezier(.25,.8,.25,1); transform: translateY(-3px) scale(1.05); padding: 2.2em 2.6em; letter-spacing: 1.2px; backdrop-filter: blur(8px); -webkit-backdrop-filter: blur(8px); border-radius: 2.5em;">
                            <span class="icon" style="margin-right: 10px;">
                              <i class="fas fa-trophy" style="color: #ffe066; font-size: 1.5em; text-shadow: 0 0 16px rgba(255,224,102,0.8), 0 0 4px #fff;"></i>
                            </span>
                            <span style="display:inline-block; vertical-align:middle;">
                              <span style="display:block; text-transform: uppercase; font-size: 1.08em; letter-spacing: 2px; color: #fff; text-shadow: 0 2px 10px rgba(24,90,157,0.18), 0 0 2px #43cea2;">
                                Accepted by
                              </span>
                              <span style="display:block; font-size:1.1em; font-weight:700; margin-top:0.15em; letter-spacing:1.5px; color:#ffe066; text-shadow: 0 2px 10px rgba(24,90,157,0.18), 0 0 2px #43cea2;">
                                ICCV 2023
                              </span>
                            </span>
                          </a>
                        </span>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Multi-Metrics_Adaptively_Identifies_Backdoors_in_Federated_Learning_ICCV_2023_paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Huang_Multi-Metrics_Adaptively_Identifies_ICCV_2023_supplemental.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/siquanhuang/Multi-metrics" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="static/pdfs/poster.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/overview.png" alt="overview" class="center-image"  style="max-width: 100%; height: auto;"/>
      <h2 class="subtitle has-text-centered">
        Overview of our defense process with step 1 and step 2 as core contributions.
      </h2>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The decentralized and privacy-preserving nature of federated learning (FL) makes it vulnerable to backdoor attacks aiming to manipulate the behavior of the resulting model on specific adversary-chosen inputs. However, most existing defenses based on statistical differences take effect only against specific attacks, especially when the malicious gradients are similar to benign ones or the data are highly non-independent and identically distributed (non-IID). In this paper, we revisit the distance-based defense methods and discover that i) Euclidean distance becomes meaningless in high dimensions and ii) malicious gradients with diverse characteristics cannot be identified by a single metric. To this end, we present a simple yet effective defense strategy with multi-metrics and dynamic weighting to identify backdoors adaptively. Furthermore, our novel defense has no reliance on predefined assumptions over attack settings or data distributions and little impact on the benign performance. To evaluate the effectiveness of our approach, we conduct comprehensive experiments on different datasets under various attack settings, where our method achieves the best defense performance. For instance, we achieve the lowest backdoor accuracy of 
3.06 % under the difficult Edge-case PGD, showing significant superiority over previous defenses. The results also demonstrate that our method can be well-adapted to a wide range of non-IID degrees without sacrificing the benign performance.
</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
         <h2 class="title is-3">Gradient Features</h2>
          <div class="level-set has-text-justified">
            <p>
              The essential logic of distance-based defense involves defining some indicative metric that can well discriminate the malicious gradients from the benign ones and removes
the hostile updates from the aggregation. Consequently, the core problem becomes how to define a metric that identifies the characteristics of hostile gradients.
          </p>
        </div>
          <div style="display: flex; justify-content: center;">
            <img src="static/images/gradient_feature.png" alt="gradient_feature" class="blend-img-background" style="max-width: 60%; height: auto;"/>
          </div>
          <p> Demonstration of  <em><strong>feature of gradient</strong></em> on two dimensional space. $\boldsymbol{w}_0$ is initial global model $\boldsymbol{w}_0$ and $\boldsymbol{w}_i$ is the local model of the $i$-th client. $\boldsymbol{w}_i - \boldsymbol{w}_0$ is the gradient and we define the \textit{feature of gradient} as $(|x+y|,l,\theta)$.</p>
        </p>
      </div>
      </div>
    </div>
  </div>
</div>
</section>


<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
         <h2 class="title is-3">Pitfall of Distance-based Defense</h2>
          <div class="level-set has-text-justified">
            <p>
              To begin with, the Euclidean distance
              suffers from the so-called “curse of dimensionality”, which renders distance metrics less sensitive in high dimensional space.
          </p>
          <br>
        </div>
      <div class="content">
        <div class="theorem-block">
          <p>
            <strong>Theorem 1 (Beyer et al. The curse of dimensionality)</strong><br>
            <span style="font-family: 'Latin Modern Math', 'Cambria Math', 'STIX Math', serif;">
              If 
              $$\lim_{d \rightarrow \infty} \mathrm{var}\left(\frac{\|X_d\|_k}{E[\|X_d\|_k]}\right) = 0,$$
              then
              $$\frac{D_{\max,d}^k - D_{\min,d}^k}{D_{\min,d}^k} \xrightarrow{p} 0,$$
              where $d$ denotes the dimensionality of the data space, $E[X]$ and $\mathrm{var}[X]$ denote the expected value and variance of a random variable $X$, $D_{\max,d}^k$ and $D_{\min,d}^k$ are the farthest/nearest distances of $N$ points to the origin using the $L_k$ distance metric.
            </span>
          </p>
        </div>
        <p>
          To find out which distance metric is more meaningful in high-dimensional space, we size up the relation between the dimension $d$ and the distance $L_k$. We have Lemma 1, which shows that $D_{\max,d}^k - D_{\min,d}^k$ increases at the ratio of $d^{(1/k)-(1/2)}$.
        </p>
        <div class="lemma-block">
          <p>
            <strong>Lemma 1 (Hinneburg et al.)</strong><br>
            <span style="font-family: 'Latin Modern Math', 'Cambria Math', 'STIX Math', serif;">
              Let $\mathcal{F}$ be an arbitrary distribution of two points and the distance function $\|\cdot\|$ be an $L_k$ metric. Then,
              $$\lim_{d \rightarrow \infty} E\left[\frac{D_{\max,d}^k - D_{\min,d}^k}{d^{(1/k)-(1/2)}}\right] = C_k,$$
              where $C_k$ is some constant dependent on $k$.
            </span>
          </p>
        </div>
        <p>
          Subsequently, we use the value of $D_{\max,d}^k - D_{\min,d}^k$ as a criterion to evaluate the ability of a particular metric to discriminate different vectors in high-dimensional space.
        </p>
        <div class="proposition-block">
          <p>
            <strong>Proposition 1</strong><br>
            <span style="font-family: 'Latin Modern Math', 'Cambria Math', 'STIX Math', serif;">
              Let $M_d = D_{\max,d}^1 - D_{\min,d}^1$ reflect the discriminating ability of Manhattan distance and $U_d = D_{\max,d}^2 - D_{\min,d}^2$ reflect the discriminating ability of Euclidean distance. Then,
              $$\lim_{d \rightarrow \infty} E\left[\frac{M_d}{U_d \cdot d^{1/2}}\right] = C',$$
              where $C'$ is a constant.
            </span>
          </p>
        </div>
        <br>
        <p>
          In light of Proposition 1 and Theorem 1, we argue that Euclidean distance is insufficient to discriminate between malicious and benign gradients. With Proposition 1, we introduce the Manhattan distance metric to describe the characteristics of gradients, which captures the difference in high-dimensional space better.
        </p>
      </div>
      </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section" id="multiple-metrics-feature">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Multiple Metrics as Feature of Gradient</h2>
    <p>
      Current defense methods are based on a single metric. With only one metric, a sophisticated attacker could effortlessly bypass them with a well-designed gradient.
    </p>
    <p>
      To this end, we propose multiple metrics to cooperatively identify the malicious gradient by defining the <em>feature of gradient</em> as the angle by the Cosine similarity, length by the Euclidean distance, and its Manhattan norm by the Manhattan distance, namely,
      
      $$\boldsymbol{x} = (x_{Man}, x_{Eul}, x_{Cosine})$$

      where $x_{Man}$ denotes the Manhattan distance $x_{Man}^{(i)} = \left\| \boldsymbol{w}_i - \boldsymbol{w}_0 \right\|_1$, $x_{Eul}$ denotes the Euclidean distance $x_{Eul}^{(i)} = \left\| \boldsymbol{w}_i - \boldsymbol{w}_0 \right\|_2$, and $x_{Cosine}$ denotes the Cosine similarity $x_{Cosine}^{(i)} = \frac{ \langle \boldsymbol{w}_0, \boldsymbol{w}_i \rangle }{ \left\| \boldsymbol{w}_0 \right\| \cdot \left\| \boldsymbol{w}_i \right\| }$.
    </p>

    <p>
      Here, $\boldsymbol{w}_0$ denotes the global model before federated training and $\boldsymbol{w}_i$ denotes the $i$-th client's local model after training. $\left\|\cdot\right\|_1$ means the $L_1$ norm of a vector, $\left\|\cdot\right\|_2$ means the $L_2$ norm of a vector, and $\langle\cdot, \cdot\rangle$ represents the inner product.
    </p>
    <p>
      After defining the <em>feature of gradient</em>, we utilize it for malicious identification. The goal is to identify the outlier among the gradients. We use the sum of the distance between each gradient as the indicator. Formally, we define it as:
    </p>
    <span style="font-family: 'Latin Modern Math', 'Cambria Math', 'STIX Math', serif;">
      \begin{equation}
          \label{redefinition}
          \begin{split}
              &\boldsymbol{x}'^{(i)} = \Bigg(
                \sum_{j,\,j \neq i}^{K} \left| x_{Man}^{(i)} - x_{Man}^{(j)} \right|
              &\qquad\qquad\sum_{j,\,j \neq i}^{K} \left| x_{Eul}^{(i)} - x_{Eul}^{(j)} \right|,
              &\qquad\qquad\sum_{j,\,j \neq i}^{K} \left| x_{Cosine}^{(i)} - x_{Cosine}^{(j)} \right|
              \Bigg)
          \end{split}
      \end{equation}
    </span>
    <p>
      This formulation enables us to capture the deviation of each client's gradient feature from the others, facilitating robust identification of malicious updates in federated learning.
    </p>
  </div>
</section>

<section class="section hero is-small is-light" id="dynamic-weighting">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Dynamic Weighting via Whitening</h2>
    <p>
      Two main challenges arise in the scoring meof each gradient: (1) the three distance metrics have different scales and are correlated, making simple normalization insufficient; (2) varying data distributions (such as different levels of non-IID) require the method to adapt dynamically for universal defense.
    </p>
    <p>
      To address these, we apply a <strong>whitening process</strong> that projects each gradient's feature onto its principal axes.
    </p>
    
      $$
        \delta^{(i)} = \sqrt{ \boldsymbol{x}'^{(i)\top} \boldsymbol{\Sigma}^{-1} \boldsymbol{x}'^{(i)} }
      $$

    <p>
      where $\boldsymbol{\Sigma}$ is the covariance matrix of all clients' features. 
    </p>
    <p>
      With the scores computed, we select gradients with higher scores, as they are less divergent and more likely to be benign. These selected gradients can then be aggregated using standard methods, such as FedAvg.
    </p>
  </div>
</section>

<section class="section" id="results" >
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Results</h2>
    <p>
      Only our method and Flame successfully resist the
      Edge-case PGD attack during the entire training process, and Flame also dampens the MA. 
    </p>

    <figure style="text-align:center;">
      <img src="static/images/cifar.png" alt="cifar_pgd" style="max-width: 80%;">
      <figcaption>The source results of various defenses against Edge-case PGD attack are in log. The figure shows MA(%) and BA(%) of various defenses under Edge-case PGD attack.</figcaption>
    </figure>

    <p>
      we show that our method obtains the highest ranking score with almost 400% better than the baseline and outperforms the second-best Flame by around 0.5.
    </p>

    <figure style="text-align:center;">
      <img src="static/images/table.png" alt="t-SNE visualization of gradient features" style="max-width: 100%;">
      <figcaption>Robustness of our approach compared to the SOTA defenses for various challenging attacks.</figcaption>
    </figure>

    <p>
      For more details and ablation studies, please refer to our <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Multi-Metrics_Adaptively_Identifies_Backdoors_in_Federated_Learning_ICCV_2023_paper.pdf">paper</a>.
    </p>
  </div>
</section>


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">Poster</h2>

      <iframe  src="static/pdfs/poster.pdf" width="100%" height="1000">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title has-text-centered">Citation</h2>
      <p>If you find our work useful in your research, please consider citing:</p>
      <pre><code>@InProceedings{Huang_2023_ICCV,
  author    = {Huang, Siquan and Li, Yijiang and Chen, Chong and Shi, Leyu and Gao, Ying},
  title     = {Multi-Metrics Adaptively Identifies Backdoors in Federated Learning},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2023},
  pages     = {4652-4662}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
